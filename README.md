Lab 7 â€” Persistance des donnÃ©es avec PV & PVC (Flask + PostgreSQL sur K3s)

 1. Objectif du projet

Ce laboratoire Ã©tend le Lab 6 en ajoutant des Persistent Volumes (PV) et Persistent Volume Claims (PVC) afin dâ€™assurer la persistance des donnÃ©es de la base de donnÃ©es PostgreSQL.

Lâ€™objectif est de conserver les donnÃ©es mÃªme si le pod de la base est supprimÃ© ou redÃ©marrÃ©.

Composants :

Frontend : Application web Flask (exposÃ©e via NodePort)

Backend : Base de donnÃ©es PostgreSQL (ClusterIP)

Persistance : PV & PVC utilisÃ©s par PostgreSQL

 2. Architecture
ğŸ”¹ Description

Lâ€™architecture se compose des Ã©lÃ©ments suivants :

Composant	Description
web-deployment	DÃ©ploiement de lâ€™application Flask
web-service	Service NodePort pour accÃ¨s externe
db-deployment	DÃ©ploiement de la base PostgreSQL
db-service	Service ClusterIP pour communication interne
PersistentVolume (PV)	Espace de stockage physique sur le nÅ“ud
PersistentVolumeClaim (PVC)	RequÃªte de stockage utilisÃ©e par PostgreSQL

ğŸ“ 3. Structure du projet
lab7/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ k8s/
|   |__ namespace.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ secret.yaml
â”‚   â”œâ”€â”€ pv.yaml
â”‚   â”œâ”€â”€ pvc.yaml
â”‚   â”œâ”€â”€ db-deployment.yaml
â”‚   â”œâ”€â”€ db-service.yaml
â”‚   â”œâ”€â”€ web-deployment.yaml
â”‚   â””â”€â”€ web-service.yaml
â”‚__docs
â””â”€â”€ README.md

âš™ï¸ 4. Ã‰tapes de dÃ©ploiement
 Ã‰tape 1 â€” CrÃ©er le ConfigMap et le Secret
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml

Ã‰tape 2 â€” CrÃ©er le PV et le PVC
kubectl apply -f pv.yaml
kubectl apply -f pvc.yaml


VÃ©rifier la crÃ©ation :

kubectl get pv
kubectl get pvc

 Ã‰tape 3 â€” DÃ©ployer la base de donnÃ©es PostgreSQL
kubectl apply -f db-deployment.yaml
kubectl apply -f db-service.yaml

 Ã‰tape 4 â€” DÃ©ployer lâ€™application Flask
kubectl apply -f web-deployment.yaml
kubectl apply -f web-service.yaml

ğŸ” 5. VÃ©rifications
Voir les pods et services :
kubectl get pods
kubectl get svc

VÃ©rifier la persistance :
kubectl get pv,pvc


ğŸŒ 6. AccÃ¨s Ã  lâ€™application

RÃ©cupÃ©rer lâ€™adresse IP du nÅ“ud :

kubectl get nodes -o wide


Ouvrir le navigateur sur :

http://<NODE_IP>:30081


Le port 30081 correspond au port NodePort du service Flask.

7. Test de persistance des donnÃ©es

Ouvre la page web du formulaire.

Saisis un nom et un email, puis clique sur Ajouter.

VÃ©rifie que les donnÃ©es sâ€™affichent dans la liste.
![Formulaire](docs/screenshots/test-lab7.1.png)

Supprime le pod de la base de donnÃ©es :

kubectl delete pod -l app=db


Une fois le pod redÃ©marrÃ©, actualise la page web â€”
âœ… Les donnÃ©es sont toujours prÃ©sentes â†’ la persistance fonctionne.
![Formulaire](docs/screenshots/test-persistance.png)

ğŸ” 8. Configuration et sÃ©curitÃ©

Les variables de configuration (DB_HOST, DB_NAME, etc.) sont stockÃ©es dans le ConfigMap.

Les identifiants sensibles (DB_USER, DB_PASSWORD) sont gÃ©rÃ©s via un Secret.

Le PVC assure que les donnÃ©es PostgreSQL sont stockÃ©es de maniÃ¨re persistante sur le disque du nÅ“ud.

âœ… 9. RÃ©sultat attendu

Lâ€™application Flask et la base PostgreSQL fonctionnent correctement sur le cluster K3s.

Les donnÃ©es restent disponibles aprÃ¨s le redÃ©marrage du pod PostgreSQL.

La configuration et les secrets sont gÃ©rÃ©s proprement via ConfigMap et Secret.

La persistance est assurÃ©e par le couple PV/PVC.
